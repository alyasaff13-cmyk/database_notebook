{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O21kEVTrOeVZ"
   },
   "source": [
    "#**Машинное обучение ИБ-2024**\n",
    "\n",
    "#**Домашнее задание 1.**\n",
    "#Регрессия, KNN, LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ2TkkhkQH-H"
   },
   "source": [
    "В данной домашней работе мы будем строить модели для предсказания цены квартиры в России. Ниже приведено описание некоторых колонок набора данных.\n",
    "\n",
    "date - дата публикации объявления\n",
    "\n",
    "price - цена в рублях\n",
    "\n",
    "level- этаж, на котором находится квартира\n",
    "\n",
    "levels - количество этажей в квартире\n",
    "\n",
    "rooms - количество комнат в квартире. Если значение -1, то квартира считается апартаментами.\n",
    "\n",
    "area - площадь квартиры.\n",
    "\n",
    "kitchen_area - площадь кухни.\n",
    "\n",
    "geo_lat - Latitude\n",
    "\n",
    "geo_lon - Longitude\n",
    "\n",
    "building_type - материал застройки. 0 - Don't know. 1 - Other. 2 - Panel. 3 - Monolithic. 4 - Brick. 5 - Blocky. 6 - Wooden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu_nZBPaRPTA"
   },
   "source": [
    "#Часть 0. Начало работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuPeg4isP_ok"
   },
   "source": [
    "Для начала работы с данными импортируем библиотеки, которые понадобятся в данном задании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AeSkfkF7_Fkp"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import sklearn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvnX1OmGRMlU"
   },
   "source": [
    "Загрузим библиотеку folium для отображения данных на карте по координатам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsEgwmPy_Pwn",
    "outputId": "f260763c-1973-4141-b506-ad27040e547d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from folium) (0.8.2)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from folium) (2.3.3)\n",
      "Requirement already satisfied: requests in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from folium) (2.32.5)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from folium) (2025.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from jinja2>=2.9->folium) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests->folium) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests->folium) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bob\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests->folium) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Igp0fvzRkGb"
   },
   "source": [
    "Распакуем наши данные из архива."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UG8salph_5nd"
   },
   "outputs": [],
   "source": [
    "#!unzip .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqWPnWJpRqhS"
   },
   "source": [
    "Загрузим данные из csv файла в датафрейм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "pHgoLbrz_Uaf",
    "outputId": "973f2842-34a6-4297-b6e6-f6834c644ba7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"input_data.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttEHZ6_FSIxd"
   },
   "source": [
    "Отобразим на карте координаты наших построек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "id": "G_pSKA2eADcf",
    "outputId": "28f228d6-881e-4b98-f368-27c3699183b8"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'geo_lat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'geo_lat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m m = folium.Map(location=[\u001b[32m55.751244\u001b[39m, \u001b[32m37.618423\u001b[39m], zoom_start=\u001b[32m10\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Список точек с широтой и долготой\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m lats = \u001b[43mmap_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgeo_lat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.loc[:\u001b[32m1000\u001b[39m]\n\u001b[32m     10\u001b[39m longs = map_df[\u001b[33m'\u001b[39m\u001b[33mgeo_lon\u001b[39m\u001b[33m'\u001b[39m].loc[:\u001b[32m1000\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Добавляем точки на карту\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'geo_lat'"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "from IPython.display import display\n",
    "\n",
    "map_df = df.loc[:1000]\n",
    "\n",
    "m = folium.Map(location=[55.751244, 37.618423], zoom_start=10)\n",
    "\n",
    "# Список точек с широтой и долготой\n",
    "lats = map_df['geo_lat'].loc[:1000]\n",
    "longs = map_df['geo_lon'].loc[:1000]\n",
    "# Добавляем точки на карту\n",
    "for point in zip(lats, longs):\n",
    "    folium.Marker(\n",
    "        location=[point[0], point[1]]\n",
    "    ).add_to(m)\n",
    "\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gibxhqRJSSr1"
   },
   "source": [
    "# Часть 1. Подготовим данные для обработки моделями машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMAFoSebSdw9"
   },
   "source": [
    "**0.5 Балл**. География наших наблюдений в наборе данных крайне большая. Однако мы знаем, что стоимость квартир в Москве и Санкт-Петербурге намного выше, чем в среднем по России. Давайте сделаем признаки, который показывают, находится ли квартира в 20 килиметрах от центра Москвы или находится ли квартира в 20 килиметрах от центра Санкт-Петербурга.\n",
    "\n",
    "Создайте два признака is_Moscow и is_Saint_Peterburg. Для нахождения расстояния по координатам используйте функцию haversine_distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pyY7M-9bBDkI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Квартир в радиусе 20 км от Москвы: 1027378\n",
      "Квартир в радиусе 20 км от Санкт-Петербурга: 875985\n"
     ]
    }
   ],
   "source": [
    "# Функция для вычисления расстояния по широте и долготе (в километрах)\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # радиус Земли, км\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    \n",
    "    a = np.sin(delta_phi / 2) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Координаты центров Москвы и Санкт-Петербурга\n",
    "moscow_center = (55.7558, 37.6173)\n",
    "saint_petersburg_center = (59.9343, 30.3351)\n",
    "\n",
    "# Создаем признаки is_Moscow и is_Saint_Peterburg\n",
    "df['is_Moscow'] = df.apply(lambda row: 1 if haversine_distance(row['geo_lat'], row['geo_lon'], moscow_center[0], moscow_center[1]) <= 20 else 0, axis=1)\n",
    "df['is_Saint_Peterburg'] = df.apply(lambda row: 1 if haversine_distance(row['geo_lat'], row['geo_lon'], saint_petersburg_center[0], saint_petersburg_center[1]) <= 20 else 0, axis=1)\n",
    "\n",
    "# Для визуализации выведем количество квартир в Москве и Питере для проверки \n",
    "print(f\"Квартир в радиусе 20 км от Москвы: {df['is_Moscow'].sum()}\")\n",
    "print(f\"Квартир в радиусе 20 км от Санкт-Петербурга: {df['is_Saint_Peterburg'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uViS3IJQUOd8"
   },
   "source": [
    "**0.5 Балла**. В нашем наборе данных есть признаки, которые мы теоретически можем использовать, например postal_code, но мы это будем делать в рамках домашней работы очень-очень долго. Поэтому предлагается удалить ненужные признаки из датафрейма.\n",
    "\n",
    "Удалим geo_lat,\tgeo_lon,\tobject_type,\tpostal_code,\tstreet_id,\tid_region,\thouse_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0wAJaXnlVFJu"
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['geo_lat', 'geo_lon', 'object_type', 'postal_code', 'street_id', 'id_region', 'house_id']\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6eOYaDCVStV"
   },
   "source": [
    "**0.5 Балл**. Для начала Вам предлагается проанализировать Ваши оставшиеся признаки (колонки) в наборе данных. Какие колонки категориальные? Какие числовые?\n",
    "\n",
    "Категориальные: (Ваш ответ)\n",
    "\n",
    "Числовые: (Ваш ответ)\n",
    "\n",
    "Давайте закодируем категориальные признаки с помощью OneHot-Encoding. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nh9Cf29oWkxw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оставшиеся колонки: ['date', 'price', 'level', 'levels', 'rooms', 'area', 'kitchen_area', 'building_type', 'is_Moscow', 'is_Saint_Peterburg']\n",
      "Категориальные признаки: ['building_type', 'rooms', 'is_Moscow', 'is_Saint_Peterburg']\n",
      "Числовые признаки: ['level', 'levels', 'area', 'kitchen_area']\n"
     ]
    }
   ],
   "source": [
    "# Определим признаки, которые остались\n",
    "print(\"Оставшиеся колонки:\", df.columns.tolist())\n",
    "\n",
    "# Предположим, что категориальные признаки — 'building_type', 'rooms', 'is_Moscow', 'is_Saint_Peterburg'\n",
    "# (rooms здесь можно считать категориальным, т.к. -1 означает апартаменты, иначе дискретное количество комнат)\n",
    "categorical_features = ['building_type', 'rooms', 'is_Moscow', 'is_Saint_Peterburg']\n",
    "\n",
    "# Числовые признаки — остальные кроме 'price' и 'date'\n",
    "numerical_features = [col for col in df.columns if col not in categorical_features + ['price', 'date']]\n",
    "\n",
    "print(\"Категориальные признаки:\", categorical_features)\n",
    "print(\"Числовые признаки:\", numerical_features)\n",
    "\n",
    "# OneHotEncoding категориальных признаков\n",
    "df = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LN3XDyceWtYw"
   },
   "source": [
    "**0.5 Балл**. Поработаем с числовыми признаками:\n",
    "\n",
    "\n",
    "1.   Добавьте в ваш датасет два признака: количество дней со дня первого наблюдения (разница между датами объявлений). Возможно, для предсказания цены не так важен этаж, как важно отношение этажа квартиры на количество этажей в доме, добавьте этот признак. После добавления нового признака колонку date можно удалить.\n",
    "2.   Числовые признаки могут иметь разные порядки. Давайте отнормируем числовые признаки с помощью StandartScaller https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Признаки добавлены, нормализованы, все значения допустимы.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Преобразуем дату в формат datetime\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')  # некорректные даты станут NaT\n",
    "\n",
    "# Удалим строки с некорректными датами\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Добавим days_since_first_observation\n",
    "min_date = df['date'].min()\n",
    "df['days_since_first_observation'] = (df['date'] - min_date).dt.days\n",
    "\n",
    "# Обработка деления при расчете floor_ratio\n",
    "# Заменим нули в 'levels' на NaN, чтобы избежать деления на 0\n",
    "df['levels'] = df['levels'].replace(0, np.nan)\n",
    "\n",
    "# Удалим строки, где levels = 0 или отсутствует (NaN)\n",
    "df = df.dropna(subset=['levels'])\n",
    "\n",
    "# Добавим признак floor_ratio\n",
    "df['floor_ratio'] = df['level'] / df['levels']\n",
    "\n",
    "# Удалим колонку 'date', она больше не нужна\n",
    "df = df.drop(columns=['date'])\n",
    "\n",
    "# --- 7. Удалим строки с любыми NaN, inf, -inf (если остались после всех шагов)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --- 8. Обновим список числовых признаков\n",
    "# (предположим, вы уже выделили категориальные отдельно)\n",
    "numerical_features = ['level', 'levels', 'area', 'kitchen_area', 'days_since_first_observation', 'floor_ratio']\n",
    "\n",
    "# --- 9. Применим StandardScaler к числовым признакам\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# --- 10. Финальная проверка: убедимся, что все значения в df теперь числовые и без NaN/inf\n",
    "assert not df[numerical_features].isna().any().any(), \"NaN остались после обработки\"\n",
    "assert np.isfinite(df[numerical_features].values).all(), \"Обнаружены inf/-inf после обработки\"\n",
    "\n",
    "print(\"✅ Признаки добавлены, нормализованы, все значения допустимы.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrYv0DFtX88C"
   },
   "source": [
    "**2 Балла**. Реализуйте класс KNNRegressor, который должен делать регрессию методом k ближайших соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Реализация класса KNNRegressor\n",
    "\n",
    "class KNNRegressor:\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "\n",
    "    # Преобразуем одномерный массив (один объект) в двумерный\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "    # Проверка на совпадение количества признаков\n",
    "        if X.shape[1] != self.X_train.shape[1]:\n",
    "            raise ValueError(f\"Количество признаков не совпадает: {X.shape[1]} vs {self.X_train.shape[1]}\")\n",
    "    \n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "            idx = np.argsort(distances)[:self.n_neighbors]\n",
    "            y_pred.append(np.mean(self.y_train[idx]))\n",
    "    \n",
    "        return np.array(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN3nHVC4ZXk_"
   },
   "source": [
    "**3 Балла**. Реализуйте класс LinearRegression, поддерживающий обучение градиентными спусками SGD, Momentum, AdaGrad. Используйте градиент для оптимизации функции потерь MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализация класса LinearRegression с методами SGD, Momentum, Adagrad\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=1000, method='SGD', batch_size=32):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.method = method\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1,1)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.zeros((n_features,1))\n",
    "        self.bias = 0\n",
    "        \n",
    "        v_theta = np.zeros((n_features,1))\n",
    "        v_bias = 0\n",
    "        G_theta = np.zeros((n_features,1))\n",
    "        G_bias = 0\n",
    "        beta = 0.9\n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            indices = np.random.choice(n_samples, self.batch_size, replace=False)\n",
    "            X_batch = X[indices]\n",
    "            y_batch = y[indices]\n",
    "            \n",
    "            y_pred = X_batch.dot(self.theta) + self.bias\n",
    "            error = y_pred - y_batch\n",
    "            \n",
    "            grad_theta = (2/self.batch_size)*X_batch.T.dot(error)\n",
    "            grad_bias = (2/self.batch_size)*np.sum(error)\n",
    "            \n",
    "            if self.method == 'SGD':\n",
    "                self.theta -= self.lr * grad_theta\n",
    "                self.bias -= self.lr * grad_bias\n",
    "            \n",
    "            elif self.method == 'Momentum':\n",
    "                v_theta = beta * v_theta + (1 - beta) * grad_theta\n",
    "                v_bias = beta * v_bias + (1 - beta) * grad_bias\n",
    "                self.theta -= self.lr * v_theta\n",
    "                self.bias -= self.lr * v_bias\n",
    "            \n",
    "            elif self.method == 'Adagrad':\n",
    "                G_theta += grad_theta**2\n",
    "                G_bias += grad_bias**2\n",
    "                self.theta -= (self.lr / (np.sqrt(G_theta) + epsilon)) * grad_theta\n",
    "                self.bias -= (self.lr / (np.sqrt(G_bias) + epsilon)) * grad_bias\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Unsupported method. Use 'SGD', 'Momentum' or 'Adagrad'\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return X.dot(self.theta) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITiFeRiITepF"
   },
   "source": [
    "# Часть 2. Эксперименты с моделями машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yjqg8pb1bSLB"
   },
   "source": [
    "**3 Балла**. Проведите эксперименты с написанными Вами методами машинного обучения. Выделите обучающую и тестовую выборки в отношении 0,8 и 0,2 соответственно. Измерьте ошибку MSE, MAE, RMSE. Заиспользуйте методы KNNRegressor и LinearRegression из библиотеки sklearn, сравните качество Ваших решений и библиотечных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "\n",
    "\n",
    "# Загрузка и подготовка данных\n",
    "\n",
    "def load_data(filepath, nrows=100000):\n",
    "    df = pd.read_csv(filepath, nrows=nrows)\n",
    "    \n",
    "    df = df.dropna(subset=['price', 'level', 'levels', 'rooms', 'area', 'kitchen_area', 'building_type'])\n",
    "    df['rooms'] = df['rooms'].replace(-1, 0)\n",
    "    df = pd.concat([df, pd.get_dummies(df['building_type'], prefix='building')], axis=1)\n",
    "    df.drop(['building_type', 'date'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Метрики для оценки качества моделей\n",
    "\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "\n",
    "#  Эксперименты с моделями\n",
    "\n",
    "\n",
    "def run_experiment(df):\n",
    "    X = df.drop('price', axis=1)\n",
    "    y = df['price'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Наш KNN\n",
    "    knn = KNNRegressor(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    \n",
    "    # Наш LinearRegression (SGD)\n",
    "    linreg = LinearRegression(lr=0.01, n_iters=1000, method='SGD', batch_size=64)\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred_linreg = linreg.predict(X_test).flatten()\n",
    "    \n",
    "    # sklearn KNN\n",
    "    sk_knn = KNeighborsRegressor(n_neighbors=5)\n",
    "    sk_knn.fit(X_train, y_train)\n",
    "    y_pred_sk_knn = sk_knn.predict(X_test)\n",
    "    \n",
    "    # sklearn LinearRegression\n",
    "    sk_linreg = SklearnLinearRegression()\n",
    "    sk_linreg.fit(X_train, y_train)\n",
    "    y_pred_sk_linreg = sk_linreg.predict(X_test)\n",
    "    \n",
    "    # Вывод результатов\n",
    "    models = {\n",
    "        'Наш KNN': y_pred_knn,\n",
    "        'Наш LinearRegression (SGD)': y_pred_linreg,\n",
    "        'sklearn KNN': y_pred_sk_knn,\n",
    "        'sklearn LinearRegression': y_pred_sk_linreg\n",
    "    }\n",
    "    \n",
    "    for name, y_pred in models.items():\n",
    "        print(f'Результаты для модели: {name}')\n",
    "        print(f'MSE: {mse(y_test, y_pred):.2f}')\n",
    "        print(f'MAE: {mae(y_test, y_pred):.2f}')\n",
    "        print(f'RMSE: {rmse(y_test, y_pred):.2f}')\n",
    "        print('-'*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.56 GiB for an array with shape (9084798, 23) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Запускаем эксперимент\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     44\u001b[39m knn = KNNRegressor(n_neighbors=\u001b[32m5\u001b[39m)\n\u001b[32m     45\u001b[39m knn.fit(X_train, y_train)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m y_pred_knn = \u001b[43mknn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Наш LinearRegression (SGD)\u001b[39;00m\n\u001b[32m     49\u001b[39m linreg = LinearRegression(lr=\u001b[32m0.01\u001b[39m, n_iters=\u001b[32m1000\u001b[39m, method=\u001b[33m'\u001b[39m\u001b[33mSGD\u001b[39m\u001b[33m'\u001b[39m, batch_size=\u001b[32m64\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mKNNRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     22\u001b[39m y_pred = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     distances = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     idx = np.argsort(distances)[:\u001b[38;5;28mself\u001b[39m.n_neighbors]\n\u001b[32m     26\u001b[39m     y_pred.append(np.mean(\u001b[38;5;28mself\u001b[39m.y_train[idx]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2828\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m add.reduce(\u001b[38;5;28mabs\u001b[39m(x), axis=axis, keepdims=keepdims)\n\u001b[32m   2826\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m == \u001b[32m2\u001b[39m:\n\u001b[32m   2827\u001b[39m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2828\u001b[39m     s = (\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m).real\n\u001b[32m   2829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n\u001b[32m   2830\u001b[39m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[32m   2831\u001b[39m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.56 GiB for an array with shape (9084798, 23) and data type object"
     ]
    }
   ],
   "source": [
    "# Запускаем эксперимент\n",
    "run_experiment(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
